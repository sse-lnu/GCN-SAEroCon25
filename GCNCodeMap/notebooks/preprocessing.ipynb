{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e_0FgnzP7pCm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiCliEhZzrjc"
   },
   "source": [
    "## 1. Repositiories of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\JABEERAK\\GNN_mapGitHUb\\GCNCodeMap\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\JABEERAK\\GNN_mapGitHUb\\GCNCodeMap\\data\\raw')\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {'jabref': r'jabref_labels.txt',\n",
    "            'sweetHome': r'sweethome3d_labels.txt',\n",
    "            'ant': r'ant_labels.txt',\n",
    "            'prom': r'prom_labels.txt',\n",
    "            'teammates': r'teammates_labels.txt',\n",
    "            'argouml': r'argouml_labels.txt',                   \n",
    "            'lucene': r'lucene_labels.txt',\n",
    "            'common': r'commons-imaging_labels.txt',        \n",
    "}\n",
    "\n",
    "roots = {\n",
    "    'jabref': 'net/sf/jabref/',\n",
    "    'sweetHome': 'com/eteks/sweethome3d/',\n",
    "    'ant': 'org/apache/tools/',\n",
    "    'prom': 'org/processmining/',\n",
    "    'teammates': 'teammates/',\n",
    "    'argouml': 'org/argouml/',\n",
    "    'lucene': 'org/apache/lucene/',\n",
    "    'common': 'org/apache/commons/imaging/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JABEERAK\\AppData\\Local\\Temp\\ipykernel_19652\\2257167341.py:151: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df.loc[df['Entity'].str.contains(file_pattern, regex=True, na=False), 'Module'] = module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(779, 6)\n",
      "(11785, 9)\n"
     ]
    }
   ],
   "source": [
    "def load_labels(labels_path):\n",
    "    \"\"\"Loads and parses the labels file into a structured dictionary.\"\"\"\n",
    "    with open(labels_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    lines = content.split('\\n')\n",
    "    data = {\n",
    "        'mapping': [],\n",
    "        'relations': [],\n",
    "        'roots': [],\n",
    "        'modules': []\n",
    "    }\n",
    "\n",
    "    current_section = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            if line.strip().startswith('# root-packages'):\n",
    "                current_section = 'roots'\n",
    "            elif line.strip().startswith('# mapping'):\n",
    "                current_section = 'mapping'\n",
    "            elif line.strip().startswith('# relations'):\n",
    "                current_section = 'relations'\n",
    "            elif line.strip().startswith('# modules'):\n",
    "                current_section = 'modules'\n",
    "            continue\n",
    "\n",
    "        if current_section == 'mapping':\n",
    "            map_file = line.split()\n",
    "            if len(map_file) == 2:\n",
    "                data['mapping'].append({'Module': map_file[0], 'Entity': map_file[1]})\n",
    "        elif current_section == 'relations':\n",
    "            source_target = line.split()\n",
    "            if len(source_target) == 2:\n",
    "                data['relations'].append({'Source': source_target[0], 'Target': source_target[1]})\n",
    "        elif current_section == 'roots':\n",
    "            if '/' in line:\n",
    "                data['roots'].append(line.strip())\n",
    "        elif current_section == 'modules':\n",
    "            data['modules'].append(line.strip())\n",
    "\n",
    "    labels = pd.DataFrame(data['mapping'])\n",
    "    labels.Entity = labels.Entity.str.replace('..', '.')\n",
    "    labels.Entity = labels.Entity.str.replace('\\\\.', '/')\n",
    "    labels.Entity = labels.Entity.str.replace('*', '.*')\n",
    "    labels['Entity'] = labels['Entity'].str.replace(r\"\\(\\?:\\?!\", r\"(?!\", regex=True)\n",
    "\n",
    "    architecture = pd.DataFrame(data['relations'])\n",
    "\n",
    "    roots = data['roots']\n",
    "    if roots:\n",
    "        root = '|'.join([r.strip('/') for r in roots])\n",
    "        if '|' in root:\n",
    "            terms = root.split('|')\n",
    "            split_terms = [term.split('/') for term in terms]\n",
    "            common_term = set(split_terms[0]).intersection(*split_terms[1:])\n",
    "            if common_term:\n",
    "                root = common_term.pop()\n",
    "    else:\n",
    "        root = ''\n",
    "    return architecture, labels, root\n",
    "\n",
    "# Method to read JSON\n",
    "def read_json(file_path, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Reads a newline-delimited JSON file, flattens it, and returns the flattened DataFrame.\n",
    "    \"\"\"\n",
    "    flattened_data_list = []\n",
    "    try:\n",
    "        with open(file_path, encoding=encoding) as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    flattened_data_list.append(pd.json_normalize(data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding line: {line} -> {e}\")\n",
    "        flattened_data = pd.concat(flattened_data_list, ignore_index=True)\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, encoding='latin-1') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    flattened_data_list.append(pd.json_normalize(data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding line: {line} -> {e}\")\n",
    "\n",
    "        flattened_data = pd.concat(flattened_data_list, ignore_index=True)\n",
    "        \n",
    "    return flattened_data\n",
    "\n",
    "# Method to clean the dataframe\n",
    "def cleaning(df, root):\n",
    "    df = df.copy()\n",
    "    df.rename(columns={'name': 'Entity'}, inplace=True)\n",
    "    df['Entity'] = df['Entity'].apply(lambda x: x.replace('.', '/'))\n",
    "    df = df[df['Entity'].str.contains(root)]\n",
    "    df.loc[:, 'Entity'] = df['Entity'].apply(lambda x: x.split('$')[0])\n",
    "    df = df[~df['Entity'].str.contains('package-info')]\n",
    "    df['File'] = df['Entity'].apply(lambda x: x.split(root)[1])\n",
    "    df['File'] = df['File'].str.lstrip('/')\n",
    "    return df\n",
    "\n",
    "# Method to extract dependencies\n",
    "def extract_dependencies(df):\n",
    "    deps_list = []\n",
    "    entity_set = set(df['Entity'])\n",
    "    for deps in df['deps']:\n",
    "        if isinstance(deps, list):\n",
    "            deps_list.extend(deps)\n",
    "    \n",
    "    deps_df = pd.DataFrame(deps_list)\n",
    "    deps_df['source'] = deps_df['source'].apply(lambda x: x.replace('.', '/'))\n",
    "    deps_df['target'] = deps_df['target'].apply(lambda x: x.replace('.', '/'))\n",
    "    \n",
    "    deps_df = deps_df[deps_df['source'].isin(entity_set) & deps_df['target'].isin(entity_set)]\n",
    "    deps_df.drop_duplicates(inplace=True)\n",
    "    deps_df.rename(columns={'source': 'Source', 'target': 'Target', 'type': 'Dependency_Type', 'count': 'Dependency_Count'}, inplace=True)\n",
    "    \n",
    "    return deps_df\n",
    "\n",
    "# Method to clean and merge the entities\n",
    "def clean_and_merge_entities(df):\n",
    "    def clean_text_tokens(texts):\n",
    "        cleaned_texts = []\n",
    "        for token in texts:\n",
    "            if re.fullmatch(r'[\\n\\t]{1,}', token):\n",
    "                continue\n",
    "            if len(token) == 1 or token.isdigit() or not any(char.isalpha() for char in token):\n",
    "                continue\n",
    "            if token in ['<init>', '<clinit>', '<p>']:\n",
    "                continue\n",
    "            token = token.replace(':', '').replace(',', '').replace('$', '')\n",
    "            cleaned_texts.append(token)\n",
    "        return cleaned_texts\n",
    "    \n",
    "    df['texts'] = df['texts'].apply(lambda x: clean_text_tokens(x) if isinstance(x, list) else [])\n",
    "    merged_df = df.groupby('Entity', as_index=False).agg({\n",
    "        'texts': lambda x: list(set(sum(x, []))),\n",
    "        **{col: 'first' for col in df.columns if col not in ['Entity', 'texts']}\n",
    "    })\n",
    "    return merged_df\n",
    "\n",
    "# Method to get module from labels\n",
    "def get_module(df, labels):\n",
    "    df = df.copy()\n",
    "    df['Module'] = None\n",
    "    for _, row in labels.iterrows():\n",
    "        file_pattern = row['Entity']\n",
    "        module = row['Module']\n",
    "        df.loc[df['Entity'].str.contains(file_pattern, regex=True, na=False), 'Module'] = module\n",
    "    \n",
    "    dff = df.copy()\n",
    "    dff['File_ID'] = range(1, len(dff) + 1)\n",
    "    dff.rename(columns={'texts': 'Code'}, inplace=True)\n",
    "    dff = dff[~dff.Module.isna()]\n",
    "    file_id_map = dict(zip(dff['Entity'], dff['File_ID']))\n",
    "    dff = dff[['File_ID', 'File', 'Entity', 'Code', 'Module']]\n",
    "    \n",
    "    return dff, file_id_map\n",
    "\n",
    "# Method to get dependencies\n",
    "def get_dependencies(file_id_map, df, df_dep, architecture):\n",
    "    df_dep = df_dep.copy()\n",
    "    df = df.copy()\n",
    "    \n",
    "    df_merged_source = pd.merge(df_dep, df[['Entity', 'Module']], left_on='Source', right_on='Entity', how='left')\n",
    "    df_merged_source = df_merged_source.rename(columns={'Module': 'Source_Module'}).drop(columns=['Entity'])\n",
    "    \n",
    "    df_merged_target = pd.merge(df_merged_source, df[['Entity', 'Module']], left_on='Target', right_on='Entity', how='left')\n",
    "    df_merged_target = df_merged_target.rename(columns={'Module': 'Target_Module'}).drop(columns=['Entity'])\n",
    "    \n",
    "    df_dep = df_merged_target.copy()\n",
    "    df_dep = df_dep[(~df_dep.Source_Module.isna()) & (~df_dep.Target_Module.isna())]\n",
    "    df_dep['Source_ID'] = df_dep['Source'].map(file_id_map)\n",
    "    df_dep['Target_ID'] = df_dep['Target'].map(file_id_map)\n",
    "    \n",
    "    df_dep = df_dep[['Source_ID', 'Source', 'Source_Module', 'Target_ID', 'Target', 'Target_Module', 'Dependency_Type', 'Dependency_Count']]\n",
    "    allowed_set = set(zip(architecture['Source'], architecture['Target']))\n",
    "    df_dep['Allowed'] = df_dep.apply(\n",
    "        lambda row: 1 if (row['Source_Module'], row['Target_Module']) in allowed_set or row['Source_Module'] == row['Target_Module']\n",
    "        else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    df_dep = df_dep[df_dep.Source != df_dep.Target]\n",
    "    df_dep.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df_dep\n",
    "def generate_Graph(df, dep):\n",
    "    modules = {file: df['Module'].iloc[i] for i, file in enumerate(df['File_ID'])}\n",
    "    nodes = {file: df['Entity'].iloc[i] for i, file in enumerate(df['File_ID'])}\n",
    "    G = nx.MultiDiGraph()\n",
    "    for file in nodes.keys():\n",
    "        G.add_node(file, code=nodes[file], label=modules[file])\n",
    "    for _, row in dep.iterrows():\n",
    "        if row['Source_ID'] in G.nodes and row['Target_ID'] in G.nodes:\n",
    "            G.add_edge(row['Source_ID'], row['Target_ID'], type=row['Dependency_Type'], weight=row['Dependency_Count'])\n",
    "            \n",
    "    df['Closeness_Centrality'] = df['File_ID'].map(nx.closeness_centrality(G))\n",
    "    return df\n",
    "\n",
    "architecture, labels, root = load_labels(labels_dict['teammates'])\n",
    "df = read_json('teammates.json')\n",
    "df = cleaning(df, root)\n",
    "df_dep = extract_dependencies(df)\n",
    "df = clean_and_merge_entities(df)\n",
    "df, file_id_map = get_module(df, labels)\n",
    "df_dep = get_dependencies(file_id_map, df, df_dep, architecture)\n",
    "df = generate_Graph(df, df_dep)\n",
    "\n",
    "print(df.shape)\n",
    "print(df_dep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making py File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --labels LABELS --json JSON --name NAME\n",
      "ipykernel_launcher.py: error: the following arguments are required: --labels, --json, --name\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JABEERAK\\.conda\\envs\\torchDL\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "def load_labels(labels_path):\n",
    "    \"\"\"Loads and parses the labels file into a structured dictionary.\"\"\"\n",
    "    with open(labels_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    lines = content.split('\\n')\n",
    "    data = {\n",
    "        'mapping': [],\n",
    "        'relations': [],\n",
    "        'roots': [],\n",
    "        'modules': []\n",
    "    }\n",
    "\n",
    "    current_section = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            if line.strip().startswith('# root-packages'):\n",
    "                current_section = 'roots'\n",
    "            elif line.strip().startswith('# mapping'):\n",
    "                current_section = 'mapping'\n",
    "            elif line.strip().startswith('# relations'):\n",
    "                current_section = 'relations'\n",
    "            elif line.strip().startswith('# modules'):\n",
    "                current_section = 'modules'\n",
    "            continue\n",
    "\n",
    "        if current_section == 'mapping':\n",
    "            map_file = line.split()\n",
    "            if len(map_file) == 2:\n",
    "                data['mapping'].append({'Module': map_file[0], 'Entity': map_file[1]})\n",
    "        elif current_section == 'relations':\n",
    "            source_target = line.split()\n",
    "            if len(source_target) == 2:\n",
    "                data['relations'].append({'Source': source_target[0], 'Target': source_target[1]})\n",
    "        elif current_section == 'roots':\n",
    "            if '/' in line:\n",
    "                data['roots'].append(line.strip())\n",
    "        elif current_section == 'modules':\n",
    "            data['modules'].append(line.strip())\n",
    "\n",
    "    labels = pd.DataFrame(data['mapping'])\n",
    "    labels.Entity = labels.Entity.str.replace('..', '.')\n",
    "    labels.Entity = labels.Entity.str.replace('\\\\.', '/')\n",
    "    labels.Entity = labels.Entity.str.replace('*', '.*')\n",
    "    labels['Entity'] = labels['Entity'].str.replace(r\"\\(\\?:\\?!\", r\"(?!\", regex=True)\n",
    "\n",
    "    architecture = pd.DataFrame(data['relations'])\n",
    "\n",
    "    roots = data['roots']\n",
    "    if roots:\n",
    "        root = '|'.join([r.strip('/') for r in roots])\n",
    "        if '|' in root:\n",
    "            terms = root.split('|')\n",
    "            split_terms = [term.split('/') for term in terms]\n",
    "            common_term = set(split_terms[0]).intersection(*split_terms[1:])\n",
    "            if common_term:\n",
    "                root = common_term.pop()\n",
    "    else:\n",
    "        root = ''\n",
    "    return architecture, labels, root\n",
    "\n",
    "def read_json(file_path, encoding='utf-8'):\n",
    "    flattened_data_list = []\n",
    "    try:\n",
    "        with open(file_path, encoding=encoding) as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    flattened_data_list.append(pd.json_normalize(data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding line: {line} -> {e}\")\n",
    "        flattened_data = pd.concat(flattened_data_list, ignore_index=True)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, encoding='latin-1') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    flattened_data_list.append(pd.json_normalize(data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding line: {line} -> {e}\")\n",
    "        flattened_data = pd.concat(flattened_data_list, ignore_index=True)\n",
    "    return flattened_data\n",
    "\n",
    "# Method to clean the dataframe\n",
    "def cleaning(df, root):\n",
    "    df = df.copy()\n",
    "    df.rename(columns={'name': 'Entity'}, inplace=True)\n",
    "    df['Entity'] = df['Entity'].apply(lambda x: x.replace('.', '/'))\n",
    "    df = df[df['Entity'].str.contains(root)]\n",
    "    df['Entity'] = df['Entity'].apply(lambda x: x.split('$')[0])\n",
    "    df = df[~df['Entity'].str.contains('package-info')]\n",
    "    df['File'] = df['Entity'].apply(lambda x: x.split(root)[1])\n",
    "    df['File'] = df['File'].str.lstrip('/')\n",
    "    return df\n",
    "\n",
    "# Method to extract dependencies\n",
    "def extract_dependencies(df):\n",
    "    deps_list = []\n",
    "    entity_set = set(df['Entity'])\n",
    "    for deps in df['deps']:\n",
    "        if isinstance(deps, list):\n",
    "            deps_list.extend(deps)\n",
    "    \n",
    "    deps_df = pd.DataFrame(deps_list)\n",
    "    deps_df['source'] = deps_df['source'].apply(lambda x: x.replace('.', '/'))\n",
    "    deps_df['target'] = deps_df['target'].apply(lambda x: x.replace('.', '/'))\n",
    "    \n",
    "    deps_df = deps_df[deps_df['source'].isin(entity_set) & deps_df['target'].isin(entity_set)]\n",
    "    deps_df.drop_duplicates(inplace=True)\n",
    "    deps_df.rename(columns={'source': 'Source', 'target': 'Target', 'type': 'Dependency_Type', 'count': 'Dependency_Count'}, inplace=True)\n",
    "    \n",
    "    return deps_df\n",
    "\n",
    "# Method to clean and merge the entities\n",
    "def clean_and_merge_entities(df):\n",
    "    def clean_text_tokens(texts):\n",
    "        cleaned_texts = []\n",
    "        for token in texts:\n",
    "            if re.fullmatch(r'[\\n\\t]{1,}', token):\n",
    "                continue\n",
    "            if len(token) == 1 or token.isdigit() or not any(char.isalpha() for char in token):\n",
    "                continue\n",
    "            if token in ['<init>', '<clinit>', '<p>']:\n",
    "                continue\n",
    "            token = token.replace(':', '').replace(',', '').replace('$', '')\n",
    "            cleaned_texts.append(token)\n",
    "        return cleaned_texts\n",
    "    \n",
    "    df['texts'] = df['texts'].apply(lambda x: clean_text_tokens(x) if isinstance(x, list) else [])\n",
    "    merged_df = df.groupby('Entity', as_index=False).agg({\n",
    "        'texts': lambda x: list(set(sum(x, []))),\n",
    "        **{col: 'first' for col in df.columns if col not in ['Entity', 'texts']}\n",
    "    })\n",
    "    return merged_df\n",
    "\n",
    "# Method to get module from labels\n",
    "def get_module(df, labels):\n",
    "    df = df.copy()\n",
    "    df['Module'] = None\n",
    "    for _, row in labels.iterrows():\n",
    "        file_pattern = row['Entity']\n",
    "        module = row['Module']\n",
    "        df.loc[df['Entity'].str.contains(file_pattern, regex=True, na=False), 'Module'] = module\n",
    "    \n",
    "    dff = df.copy()\n",
    "    dff['File_ID'] = range(1, len(dff) + 1)\n",
    "    dff.rename(columns={'texts': 'Code'}, inplace=True)\n",
    "    dff = dff[~dff.Module.isna()]\n",
    "    file_id_map = dict(zip(dff['Entity'], dff['File_ID']))\n",
    "    dff = dff[['File_ID', 'File', 'Entity', 'Code', 'Module']]\n",
    "    \n",
    "    return dff, file_id_map\n",
    "\n",
    "# Method to get dependencies\n",
    "def get_dependencies(file_id_map, df, df_dep, architecture):\n",
    "    df_dep = df_dep.copy()\n",
    "    df = df.copy()\n",
    "    \n",
    "    df_merged_source = pd.merge(df_dep, df[['Entity', 'Module']], left_on='Source', right_on='Entity', how='left')\n",
    "    df_merged_source = df_merged_source.rename(columns={'Module': 'Source_Module'}).drop(columns=['Entity'])\n",
    "    \n",
    "    df_merged_target = pd.merge(df_merged_source, df[['Entity', 'Module']], left_on='Target', right_on='Entity', how='left')\n",
    "    df_merged_target = df_merged_target.rename(columns={'Module': 'Target_Module'}).drop(columns=['Entity'])\n",
    "    \n",
    "    df_dep = df_merged_target.copy()\n",
    "    df_dep = df_dep[(~df_dep.Source_Module.isna()) & (~df_dep.Target_Module.isna())]\n",
    "    df_dep['Source_ID'] = df_dep['Source'].map(file_id_map)\n",
    "    df_dep['Target_ID'] = df_dep['Target'].map(file_id_map)\n",
    "    \n",
    "    df_dep = df_dep[['Source_ID', 'Source', 'Source_Module', 'Target_ID', 'Target', 'Target_Module', 'Dependency_Type', 'Dependency_Count']]\n",
    "    allowed_set = set(zip(architecture['Source'], architecture['Target']))\n",
    "    df_dep['Allowed'] = df_dep.apply(\n",
    "        lambda row: 1 if (row['Source_Module'], row['Target_Module']) in allowed_set or row['Source_Module'] == row['Target_Module']\n",
    "        else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    df_dep = df_dep[df_dep.Source != df_dep.Target]\n",
    "    df_dep.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df_dep\n",
    "def generate_Graph(df, dep):\n",
    "    modules = {file: df['Module'].iloc[i] for i, file in enumerate(df['File_ID'])}\n",
    "    nodes = {file: df['Entity'].iloc[i] for i, file in enumerate(df['File_ID'])}\n",
    "    G = nx.MultiDiGraph()\n",
    "    for file in nodes.keys():\n",
    "        G.add_node(file, code=nodes[file], label=modules[file])\n",
    "    for _, row in dep.iterrows():\n",
    "        if row['Source_ID'] in G.nodes and row['Target_ID'] in G.nodes:\n",
    "            G.add_edge(row['Source_ID'], row['Target_ID'], type=row['Dependency_Type'], weight=row['Dependency_Count'])\n",
    "            \n",
    "    df['Closeness_Centrality'] = df['File_ID'].map(nx.closeness_centrality(G))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def main(labels_path, json_path, dataset_name):\n",
    "\n",
    "    architecture, labels, root = load_labels(labels_path)\n",
    "    df = read_json(json_path)\n",
    "    df = cleaning(df, root)\n",
    "    df_dep = extract_dependencies(df)\n",
    "    df = clean_and_merge_entities(df)\n",
    "    df, file_id_map = get_module(df, labels)\n",
    "    df_dep = get_dependencies(file_id_map, df, df_dep, architecture)\n",
    "    df = generate_Graph(df, df_dep)\n",
    "    processed_dir = os.path.join(os.path.dirname(labels_path), \"processed\")\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    df.to_csv(os.path.join(processed_dir, f'df_{dataset_name}.csv'), index=False)\n",
    "    df_dep.to_csv(os.path.join(processed_dir, f'dep_{dataset_name}.csv'), index=False)\n",
    "    print(f\"Processing complete. Results saved in '{processed_dir}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Process JSON and labels files.\")\n",
    "    parser.add_argument('--labels', required=True, help=\"Path to the labels file\")\n",
    "    parser.add_argument('--json', required=True, help=\"Path to the JSON file\")\n",
    "    parser.add_argument('--name', required=True, help=\"Name of the dataset (used for output files)\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args.labels, args.json, args.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torchDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
