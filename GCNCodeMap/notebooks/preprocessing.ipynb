{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e_0FgnzP7pCm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiCliEhZzrjc"
   },
   "source": [
    "## 1. Repositiories of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\JABEERAK\\GNN_CodeMapping\\GCNCodeMap\\Data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\JABEERAK\\GNN_CodeMapping\\GCNCodeMap\\Data')\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {'jabref': r'jabref_labels.txt',\n",
    "            'sweetHome': r'sweethome3d_labels.txt',\n",
    "            'ant': r'ant_labels.txt',\n",
    "            'prom': r'prom_labels.txt',\n",
    "            'teammates': r'teammates_labels.txt',\n",
    "            'argouml': r'argouml_labels.txt',                   \n",
    "            'lucene': r'lucene_labels.txt',\n",
    "            'common': r'commons-imaging_labels.txt',        \n",
    "}\n",
    "\n",
    "roots = {\n",
    "    'jabref': 'net/sf/jabref/',\n",
    "    'sweetHome': 'com/eteks/sweethome3d/',\n",
    "    'ant': 'org/apache/tools/',\n",
    "    'prom': 'org/processmining/',\n",
    "    'teammates': 'teammates/',\n",
    "    'argouml': 'org/argouml/',\n",
    "    'lucene': 'org/apache/lucene/',\n",
    "    'common': 'org/apache/commons/imaging/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(labels_path):\n",
    "    \"\"\"Loads and parses the labels file into a structured dictionary.\"\"\"\n",
    "    with open(labels_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    lines = content.split('\\n')\n",
    "    data = {\n",
    "        'mapping': [],\n",
    "        'relations': [],\n",
    "        'roots': [],\n",
    "        'modules': [] \n",
    "    }\n",
    "\n",
    "    current_section = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            if line.strip().startswith('# root-packages'):\n",
    "                current_section = 'roots'\n",
    "            elif line.strip().startswith('# mapping'):\n",
    "                current_section = 'mapping'\n",
    "            elif line.strip().startswith('# relations'):\n",
    "                current_section = 'relations'\n",
    "            elif line.strip().startswith('# modules'): \n",
    "                current_section = 'modules'\n",
    "            continue\n",
    "\n",
    "        if current_section == 'mapping':\n",
    "            map_file = line.split()\n",
    "            if len(map_file) == 2:\n",
    "                data['mapping'].append({'Module': map_file[0], 'Entity': map_file[1]})\n",
    "        elif current_section == 'relations':\n",
    "            source_target = line.split()\n",
    "            if len(source_target) == 2:\n",
    "                data['relations'].append({'Source': source_target[0], 'Target': source_target[1]})\n",
    "        elif current_section == 'roots':\n",
    "            \n",
    "            if '/' in line: \n",
    "                data['roots'].append(line.strip())\n",
    "        elif current_section == 'modules':\n",
    "            data['modules'].append(line.strip())\n",
    "\n",
    "    labels = pd.DataFrame(data['mapping'])\n",
    "    labels.Entity = labels.Entity.str.replace('..', '.')\n",
    "    labels.Entity = labels.Entity.str.replace('\\\\.', '/')\n",
    "    labels.Entity = labels.Entity.str.replace('*', '.*')\n",
    "    labels['Entity'] = labels['Entity'].str.replace(r\"\\(\\?:\\?!\", r\"(?!\", regex=True )\n",
    "    \n",
    "    architecture = pd.DataFrame(data['relations'])\n",
    "    roots = data['roots']\n",
    "    if len(roots) == 1:\n",
    "        root = roots[0].strip('/')\n",
    "    else:\n",
    "        root = ''\n",
    "\n",
    "    return architecture, labels, root\n",
    "\n",
    "\n",
    "architecture, labels, root = load_labels(labels_dict['jabref'])\n",
    "\n",
    "def read_and_flatten_json(file_path, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Reads a newline-delimited JSON file, flattens it, and returns the flattened DataFrame.\n",
    "    \"\"\"\n",
    "    flattened_data_list = []\n",
    "    try:\n",
    "        with open(file_path, encoding=encoding) as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    flattened_data_list.append(pd.json_normalize(data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding line: {line} -> {e}\")\n",
    "        flattened_data = pd.concat(flattened_data_list, ignore_index=True)\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, encoding='latin-1') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    flattened_data_list.append(pd.json_normalize(data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding line: {line} -> {e}\")\n",
    "\n",
    "        flattened_data = pd.concat(flattened_data_list, ignore_index=True)\n",
    "        \n",
    "    return flattened_data\n",
    "\n",
    "df = read_and_flatten_json('jabref.json')\n",
    "\n",
    "def cleaning(df, root):\n",
    "    df=df.copy()\n",
    "    df.rename(columns={'name': 'Entity'}, inplace=True)\n",
    "    df.Entity = df.Entity.apply(lambda x: x.replace('.','/'))\n",
    "    df = df[df.Entity.str.contains(root)]\n",
    "    df.loc[:, 'Entity'] = df['Entity'].apply(lambda x: x.split('$')[0])\n",
    "    df = df[~df.Entity.str.contains('package-info')]\n",
    "    df['File'] = df.Entity.apply(lambda x: x.split(root)[1])\n",
    "    df['File'] = df['File'].str.lstrip('/')\n",
    "    return df\n",
    "\n",
    "architecture, labels, root = load_labels(labels_dict['jabref'])\n",
    "\n",
    "df = cleaning(df, root)\n",
    "\n",
    "def extract_and_filter_dependencies(df):\n",
    "    deps_list = []\n",
    "    entity_set = set(df['Entity'])\n",
    "    \n",
    "    for deps in df['deps']:\n",
    "        if isinstance(deps, list):\n",
    "            deps_list.extend(deps)\n",
    "\n",
    "    # Create DataFrame from dependencies\n",
    "    deps_df = pd.DataFrame(deps_list)\n",
    "    deps_df['source'] = deps_df['source'].apply(lambda x: x.replace('.','/'))\n",
    "    deps_df['target'] = deps_df['target'].apply(lambda x: x.replace('.','/'))\n",
    "\n",
    "    deps_df = deps_df[deps_df['source'].isin(entity_set) & deps_df['target'].isin(entity_set)]\n",
    "    deps_df.drop_duplicates(inplace=True)\n",
    "    deps_df.rename(columns={'source': 'Source', 'target': 'Target', 'type': 'Dependency_Type', 'count': 'Dependency_Count'}, inplace=True)\n",
    "    \n",
    "    return deps_df\n",
    "\n",
    "df_dep = extract_and_filter_dependencies(df)\n",
    "\n",
    "\n",
    "def clean_and_merge_entities(df):\n",
    "    # Step 1: Clean the 'texts' column\n",
    "    def clean_text_tokens(texts):\n",
    "        cleaned_texts = []\n",
    "        for token in texts:\n",
    "            if re.fullmatch(r'[\\n\\t]{1,}', token):  # Matches tokens like [\\n], [\\n\\n], or [\\t]\n",
    "                continue\n",
    "            if len(token) == 1:\n",
    "                continue\n",
    "            if token.isdigit() or not any(char.isalpha() for char in token):\n",
    "                continue\n",
    "            if token in ['<init>', '<clinit>', '<p>']:\n",
    "                continue\n",
    "            token = token.replace(':', '')\n",
    "            token = token.replace(',', '')\n",
    "            token = token.replace('$', '')\n",
    "            cleaned_texts.append(token)\n",
    "\n",
    "        return cleaned_texts\n",
    "\n",
    "    df['texts'] = df['texts'].apply(lambda x: clean_text_tokens(x) if isinstance(x, list) else [])\n",
    "\n",
    "    merged_df = (\n",
    "        df.groupby('Entity', as_index=False)\n",
    "        .agg({\n",
    "            'texts': lambda x: list(set(sum(x, []))),  # Deduplicate and merge text lists\n",
    "            **{col: 'first' for col in df.columns if col not in ['Entity', 'texts']}\n",
    "        })\n",
    "    )\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "df = clean_and_merge_entities(df)\n",
    "\n",
    "def get_module(df, labels):\n",
    "    df=df.copy()\n",
    "    df['Module'] = None\n",
    "    for _, row in labels.iterrows():\n",
    "        file_pattern = row['Entity']\n",
    "        module = row['Module']\n",
    "        df.loc[df['Entity'].str.contains(file_pattern, regex=True, na=False), 'Module'] = module\n",
    "        \n",
    "    dff = df.copy()\n",
    "    dff['File_ID'] = range(1, len(dff) + 1)\n",
    "    dff.rename(columns={'texts': 'Code'}, inplace=True)\n",
    "    dff = dff[~dff.Module.isna()]\n",
    "    file_id_map = dict(zip(dff['Entity'], dff['File_ID']))\n",
    "    dff = dff[['File_ID','File', 'Entity',\t'Code','Module']]\n",
    "    return dff, file_id_map\n",
    "\n",
    "dff, file_id_map =  get_module(df,labels)\n",
    "\n",
    "def get_dependencies(file_id_map,df, df_dep):\n",
    "    df_dep = df_dep.copy()\n",
    "    df = df.copy()\n",
    "\n",
    "    df_merged_source = pd.merge(df_dep, df[['Entity', 'Module']], left_on='Source', right_on='Entity', how='left')\n",
    "    df_merged_source = df_merged_source.rename(columns={'Module': 'Source_Module'}).drop(columns=['Entity'])\n",
    "\n",
    "    df_merged_target = pd.merge(df_merged_source, df[['Entity', 'Module']], left_on='Target', right_on='Entity', how='left')\n",
    "    df_merged_target = df_merged_target.rename(columns={'Module': 'Target_Module'}).drop(columns=['Entity'])\n",
    "\n",
    "    df_dep = df_merged_target.copy()\n",
    "    df_dep = df_dep[(~df_dep.Source_Module.isna()) & (~df_dep.Target_Module.isna())]\n",
    "    df_dep['Source_ID'] = df_dep['Source'].map(file_id_map)\n",
    "    df_dep['Target_ID'] = df_dep['Target'].map(file_id_map)\n",
    "\n",
    "    df_dep = df_dep[['Source_ID', 'Source', 'Source_Module','Target_ID', 'Target', 'Target_Module', 'Dependency_Type', 'Dependency_Count']]\n",
    "    allowed_set = set(zip(architecture['Source'], architecture['Target']))\n",
    "    df_dep['Allowed'] = df_dep.apply(\n",
    "        lambda row: 1 if (row['Source_Module'], row['Target_Module']) in allowed_set or row['Source_Module'] == row['Target_Module']\n",
    "        else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    df_dep = df_dep[df_dep.Source != df_dep.Target]\n",
    "    df_dep.drop_duplicates(inplace=True)\n",
    "    return df_dep\n",
    "\n",
    "df_dep = get_dependencies(file_id_map,dff, df_dep)\n",
    "\n",
    "def generate_Graph(df, dep):\n",
    "    modules = {file: df['Module'].iloc[i] for i, file in enumerate(df['File_ID'])}\n",
    "    nodes = {file: df['Entity'].iloc[i] for i, file in enumerate(df['File_ID'])}\n",
    "    G = nx.MultiDiGraph()\n",
    "    for file in nodes.keys():\n",
    "        G.add_node(file, code=nodes[file], label=modules[file])\n",
    "    for _, row in dep.iterrows():\n",
    "        if row['Source_ID'] in G.nodes and row['Target_ID'] in G.nodes:\n",
    "            G.add_edge(row['Source_ID'], row['Target_ID'], type=row['Dependency_Type'], weight=row['Dependency_Count'])\n",
    "            \n",
    "    df['Closeness_Centrality'] = df['File_ID'].map(nx.closeness_centrality(G))\n",
    "    return df\n",
    "\n",
    "dff = generate_Graph(dff,df_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Code</th>\n",
       "      <th>Module</th>\n",
       "      <th>Closeness_Centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Globals</td>\n",
       "      <td>net/sf/jabref/Globals</td>\n",
       "      <td>[IMPORT_FORMAT_READER, streamEavesdropper, get...</td>\n",
       "      <td>globals</td>\n",
       "      <td>0.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>JabRefException</td>\n",
       "      <td>net/sf/jabref/JabRefException</td>\n",
       "      <td>[getLocalizedMessage, Lnet/sf/jabref/JabRefExc...</td>\n",
       "      <td>model</td>\n",
       "      <td>0.123622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>JabRefExecutorService</td>\n",
       "      <td>net/sf/jabref/JabRefExecutorService</td>\n",
       "      <td>[command, execute, x1,  - low prio, LOGGER, Ja...</td>\n",
       "      <td>logic</td>\n",
       "      <td>0.101804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>JabRefGUI</td>\n",
       "      <td>net/sf/jabref/JabRefGUI</td>\n",
       "      <td>[java.runtime.name, argsDatabases, &lt;/html&gt;, wi...</td>\n",
       "      <td>gui</td>\n",
       "      <td>0.096597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>JabRefMain</td>\n",
       "      <td>net/sf/jabref/JabRefMain</td>\n",
       "      <td>[preferences, numericFields, proxyPreferences,...</td>\n",
       "      <td>gui</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1015</td>\n",
       "      <td>shared/exception/SharedEntryNotPresentException</td>\n",
       "      <td>net/sf/jabref/shared/exception/SharedEntryNotP...</td>\n",
       "      <td>[getNonPresentBibEntry, nonPresentBibEntry, Re...</td>\n",
       "      <td>logic</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>1016</td>\n",
       "      <td>shared/listener/OracleNotificationListener</td>\n",
       "      <td>net/sf/jabref/shared/listener/OracleNotificati...</td>\n",
       "      <td>[dbmsSynchronizer, event, onDatabaseChangeNoti...</td>\n",
       "      <td>logic</td>\n",
       "      <td>0.066093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1017</td>\n",
       "      <td>shared/listener/PostgresSQLNotificationListener</td>\n",
       "      <td>net/sf/jabref/shared/listener/PostgresSQLNotif...</td>\n",
       "      <td>[dbmsSynchronizer, payload, channel, notificat...</td>\n",
       "      <td>logic</td>\n",
       "      <td>0.066062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1018</td>\n",
       "      <td>shared/prefs/SharedDatabasePreferences</td>\n",
       "      <td>net/sf/jabref/shared/prefs/SharedDatabasePrefe...</td>\n",
       "      <td>[password, rememberPassword, setType, sharedDa...</td>\n",
       "      <td>preferences</td>\n",
       "      <td>0.096625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1019</td>\n",
       "      <td>shared/security/Password</td>\n",
       "      <td>net/sf/jabref/shared/security/Password</td>\n",
       "      <td>[ThisIsA128BitKey, secretKey, ivSpec, cipher, ...</td>\n",
       "      <td>model</td>\n",
       "      <td>0.082077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1015 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      File_ID                                             File  \\\n",
       "0           1                                          Globals   \n",
       "1           2                                  JabRefException   \n",
       "2           3                            JabRefExecutorService   \n",
       "3           4                                        JabRefGUI   \n",
       "4           5                                       JabRefMain   \n",
       "...       ...                                              ...   \n",
       "1014     1015  shared/exception/SharedEntryNotPresentException   \n",
       "1015     1016       shared/listener/OracleNotificationListener   \n",
       "1016     1017  shared/listener/PostgresSQLNotificationListener   \n",
       "1017     1018           shared/prefs/SharedDatabasePreferences   \n",
       "1018     1019                         shared/security/Password   \n",
       "\n",
       "                                                 Entity  \\\n",
       "0                                 net/sf/jabref/Globals   \n",
       "1                         net/sf/jabref/JabRefException   \n",
       "2                   net/sf/jabref/JabRefExecutorService   \n",
       "3                               net/sf/jabref/JabRefGUI   \n",
       "4                              net/sf/jabref/JabRefMain   \n",
       "...                                                 ...   \n",
       "1014  net/sf/jabref/shared/exception/SharedEntryNotP...   \n",
       "1015  net/sf/jabref/shared/listener/OracleNotificati...   \n",
       "1016  net/sf/jabref/shared/listener/PostgresSQLNotif...   \n",
       "1017  net/sf/jabref/shared/prefs/SharedDatabasePrefe...   \n",
       "1018             net/sf/jabref/shared/security/Password   \n",
       "\n",
       "                                                   Code       Module  \\\n",
       "0     [IMPORT_FORMAT_READER, streamEavesdropper, get...      globals   \n",
       "1     [getLocalizedMessage, Lnet/sf/jabref/JabRefExc...        model   \n",
       "2     [command, execute, x1,  - low prio, LOGGER, Ja...        logic   \n",
       "3     [java.runtime.name, argsDatabases, </html>, wi...          gui   \n",
       "4     [preferences, numericFields, proxyPreferences,...          gui   \n",
       "...                                                 ...          ...   \n",
       "1014  [getNonPresentBibEntry, nonPresentBibEntry, Re...        logic   \n",
       "1015  [dbmsSynchronizer, event, onDatabaseChangeNoti...        logic   \n",
       "1016  [dbmsSynchronizer, payload, channel, notificat...        logic   \n",
       "1017  [password, rememberPassword, setType, sharedDa...  preferences   \n",
       "1018  [ThisIsA128BitKey, secretKey, ivSpec, cipher, ...        model   \n",
       "\n",
       "      Closeness_Centrality  \n",
       "0                 0.126800  \n",
       "1                 0.123622  \n",
       "2                 0.101804  \n",
       "3                 0.096597  \n",
       "4                 0.000000  \n",
       "...                    ...  \n",
       "1014              0.000000  \n",
       "1015              0.066093  \n",
       "1016              0.066062  \n",
       "1017              0.096625  \n",
       "1018              0.082077  \n",
       "\n",
       "[1015 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making py File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python reprocess.py <labels_path> <json_path> <dataset_name>\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def load_labels(labels_path):\n",
    "    \"\"\"Loads and parses the labels file into a structured dictionary.\"\"\"\n",
    "    with open(labels_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    lines = content.split('\\n')\n",
    "    data = {\n",
    "        'mapping': [],\n",
    "        'relations': [],\n",
    "        'roots': [],\n",
    "        'modules': [] \n",
    "    }\n",
    "\n",
    "    current_section = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            if line.strip().startswith('# root-packages'):\n",
    "                current_section = 'roots'\n",
    "            elif line.strip().startswith('# mapping'):\n",
    "                current_section = 'mapping'\n",
    "            elif line.strip().startswith('# relations'):\n",
    "                current_section = 'relations'\n",
    "            elif line.strip().startswith('# modules'): \n",
    "                current_section = 'modules'\n",
    "            continue\n",
    "\n",
    "        if current_section == 'mapping':\n",
    "            map_file = line.split()\n",
    "            if len(map_file) == 2:\n",
    "                data['mapping'].append({'Module': map_file[0], 'Entity': map_file[1]})\n",
    "        elif current_section == 'relations':\n",
    "            source_target = line.split()\n",
    "            if len(source_target) == 2:\n",
    "                data['relations'].append({'Source': source_target[0], 'Target': source_target[1]})\n",
    "        elif current_section == 'roots':\n",
    "            \n",
    "            if '/' in line: \n",
    "                data['roots'].append(line.strip())\n",
    "        elif current_section == 'modules':\n",
    "            data['modules'].append(line.strip())\n",
    "\n",
    "    labels = pd.DataFrame(data['mapping'])\n",
    "    labels.Entity = labels.Entity.str.replace('..', '.')\n",
    "    labels.Entity = labels.Entity.str.replace('\\\\.', '/')\n",
    "    labels.Entity = labels.Entity.str.replace('*', '.*')\n",
    "    labels['Entity'] = labels['Entity'].str.replace(r\"\\(\\?:\\?!\", r\"(?!\", regex=True )\n",
    "    \n",
    "    architecture = pd.DataFrame(data['relations'])\n",
    "    roots = data['roots']\n",
    "    if len(roots) == 1:\n",
    "        root = roots[0].strip('/')\n",
    "    else:\n",
    "        root = ''\n",
    "\n",
    "    return architecture, labels, root\n",
    "\n",
    "\n",
    "def read_and_process_json(file_path, root):\n",
    "    \"\"\"Reads and cleans the JSON file, extracting relevant dependencies and entities.\"\"\"\n",
    "    flattened_data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    flattened_data.append(pd.json_normalize(data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding line: {line} -> {e}\")\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, encoding='latin-1') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    flattened_data.append(pd.json_normalize(data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding line: {line} -> {e}\")\n",
    "\n",
    "    df = pd.concat(flattened_data, ignore_index=True)\n",
    "    df = df.rename(columns={'name': 'Entity'}).copy()\n",
    "    df['Entity'] = df['Entity'].str.replace('.', '/')\n",
    "    df = df[df['Entity'].str.contains(root)]\n",
    "    df['Entity'] = df['Entity'].str.split('$').str[0]\n",
    "    df = df[~df['Entity'].str.contains('package-info')]\n",
    "    df['File'] = df['Entity'].str.split(root).str[1]\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_merge(df, labels):\n",
    "    \"\"\"Cleans and merges entities, then assigns modules.\"\"\"\n",
    "    def clean_tokens(texts):\n",
    "        return [token for token in texts if len(token) > 1 and token.isalnum()]\n",
    "\n",
    "    df['texts'] = df['texts'].apply(lambda x: clean_tokens(x) if isinstance(x, list) else [])\n",
    "    df['Module'] = None\n",
    "    for _, row in labels.iterrows():\n",
    "        df.loc[df['Entity'].str.contains(row['Entity'], regex=True, na=False), 'Module'] = row['Module']\n",
    "\n",
    "    df['File_ID'] = range(1, len(df) + 1)\n",
    "    df = df[~df['Module'].isna()]\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_dependencies(df, df_dep, architecture):\n",
    "    \"\"\"Extracts and filters dependencies based on modules and architecture.\"\"\"\n",
    "    entity_set = set(df['Entity'])\n",
    "    df_dep = pd.DataFrame(df_dep)\n",
    "    df_dep['source'] = df_dep['source'].str.replace('.', '/')\n",
    "    df_dep['target'] = df_dep['target'].str.replace('.', '/')\n",
    "    df_dep = df_dep[df_dep['source'].isin(entity_set) & df_dep['target'].isin(entity_set)]\n",
    "    df_dep = pd.merge(df_dep, df[['Entity', 'Module']], left_on='source', right_on='Entity', how='left').rename(columns={'Module': 'Source_Module'})\n",
    "    df_dep = pd.merge(df_dep, df[['Entity', 'Module']], left_on='target', right_on='Entity', how='left').rename(columns={'Module': 'Target_Module'})\n",
    "    allowed_set = set(zip(architecture['Source'], architecture['Target']))\n",
    "    df_dep['Allowed'] = df_dep.apply(lambda row: 1 if (row['Source_Module'], row['Target_Module']) in allowed_set or row['Source_Module'] == row['Target_Module'] else 0, axis=1)\n",
    "    return df_dep\n",
    "\n",
    "def generate_Graph(df, dep):\n",
    "    \"\"\"Generates a graph and computes closeness centrality.\"\"\"\n",
    "    modules = {file: df['Module'].iloc[i] for i, file in enumerate(df['File_ID'])}\n",
    "    nodes = {file: df['Entity'].iloc[i] for i, file in enumerate(df['File_ID'])}\n",
    "    G = nx.MultiDiGraph()\n",
    "    for file in nodes.keys():\n",
    "        G.add_node(file, code=nodes[file], label=modules[file])\n",
    "    for _, row in dep.iterrows():\n",
    "        if row['Source_ID'] in G.nodes and row['Target_ID'] in G.nodes:\n",
    "            G.add_edge(row['Source_ID'], row['Target_ID'], type=row['Dependency_Type'], weight=row['Dependency_Count'])\n",
    "            \n",
    "    df['Closeness_Centrality'] = df['File_ID'].map(nx.closeness_centrality(G))\n",
    "    return df\n",
    "\n",
    "\n",
    "def main(labels_path, json_path, dataset_name):\n",
    "    \"\"\"Main processing pipeline.\"\"\"\n",
    "    architecture, labels, root = load_labels(labels_path)\n",
    "    df = read_and_process_json(json_path, root)\n",
    "    df = clean_and_merge(df, labels)\n",
    "    df_dep = extract_dependencies(df, df['deps'], architecture)\n",
    "    df = generate_Graph(df, df_dep)\n",
    "\n",
    "    # Ensure processed directory exists\n",
    "    processed_dir = os.path.join(\"data\", \"processed\")\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "    # Save processed data in the processed directory\n",
    "    df.to_csv(os.path.join(processed_dir, f'{dataset_name}_entities.csv'), index=False)\n",
    "    df_dep.to_csv(os.path.join(processed_dir, f'{dataset_name}_dependencies.csv'), index=False)\n",
    "    print(f\"Processing complete. Results saved in '{processed_dir}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 4:\n",
    "        print(\"Usage: python reprocess.py <labels_path> <json_path> <dataset_name>\")\n",
    "        sys.exit(1)\n",
    "    labels_path = sys.argv[1]\n",
    "    json_path = sys.argv[2]\n",
    "    dataset_name = sys.argv[3]\n",
    "    main(labels_path, json_path, dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --labels LABELS --json JSON --name NAME\n",
      "ipykernel_launcher.py: error: the following arguments are required: --labels, --json, --name\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Process JSON and labels files.\")\n",
    "    parser.add_argument('--labels', required=True, help=\"Path to the labels file\")\n",
    "    parser.add_argument('--json', required=True, help=\"Path to the JSON file\")\n",
    "    parser.add_argument('--name', required=True, help=\"name of the dataset\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.labels, args.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
